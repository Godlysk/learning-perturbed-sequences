{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceLearner:\n",
    "\n",
    "    def __init__(self, n_sensory, n_neurons, top_k, p, scaffold=False):\n",
    "        self.epsilon = 1e-8\n",
    "        self.ns = n_sensory\n",
    "        self.nn = n_neurons\n",
    "        self.top_k = top_k\n",
    "        self.p = p\n",
    "        self.scaffold = scaffold\n",
    "        if (self.scaffold):\n",
    "            self.setup_scaffold()\n",
    "        else:\n",
    "            self.setup_simple()\n",
    "        self.renormalize()\n",
    "        \n",
    "    def setup_simple(self):\n",
    "        # setup a simple brain\n",
    "        self.n_areas = 1\n",
    "        self.brain_size = self.ns + self.nn * self.n_areas\n",
    "        self.W = np.zeros((self.brain_size, self.brain_size))\n",
    "        # sensory -> output (area 1)    [afferent]\n",
    "        self.W[:self.ns, self.ns:] = np.random.choice([0.0, 1.0], size=(self.ns, self.nn), p=[1 - self.p, self.p])\n",
    "        # output (area 1)   [recurrent]\n",
    "        self.W[self.ns:, self.ns:] = np.random.choice([0.0, 1.0], size=(self.nn, self.nn), p=[1 - self.p, self.p])\n",
    "        # remove any self connections\n",
    "        np.fill_diagonal(self.W, 0.0)\n",
    "        self.inputs = np.zeros(self.brain_size - self.ns)\n",
    "\n",
    "    def setup_scaffold(self):\n",
    "        # setup a scaffold brain\n",
    "        self.n_areas = 2\n",
    "        self.brain_size = self.ns + self.nn * self.n_areas\n",
    "        self.W = np.zeros((self.brain_size, self.brain_size))\n",
    "        # sensory -> output (area 1)    [afferent]\n",
    "        self.W[:self.ns, self.ns:self.ns + self.nn] = np.random.choice([0.0, 1.0], size=(self.ns, self.nn), p=[1 - self.p, self.p])\n",
    "        # output -> scaffold (area 2)   [afferent]\n",
    "        self.W[self.ns:self.ns + self.nn, self.ns + self.nn:] = np.random.choice([0.0, 1.0], size=(self.nn, self.nn), p=[1 - self.p, self.p])\n",
    "        # scaffold (area 2) -> output   [afferent]\n",
    "        self.W[self.ns + self.nn:, self.ns:self.ns + self.nn] = np.random.choice([0.0, 1.0], size=(self.nn, self.nn), p=[1 - self.p, self.p])\n",
    "        # output (area 1)   [recurrent]\n",
    "        self.W[self.ns:self.ns + self.nn, self.ns:self.ns + self.nn] = np.random.choice([0.0, 1.0], size=(self.nn, self.nn), p=[1 - self.p, self.p])\n",
    "        # scaffold (area 2)   [recurrent]\n",
    "        self.W[self.ns + self.nn:, self.ns + self.nn:] = np.random.choice([0.0, 1.0], size=(self.nn, self.nn), p=[1 - self.p, self.p])\n",
    "        # remove any self connections\n",
    "        np.fill_diagonal(self.W, 0.0)   \n",
    "        self.inputs = np.zeros(self.brain_size - self.ns)\n",
    "\n",
    "    def top_k_indices(self, array, offset=0):\n",
    "        top_indices = [ ]\n",
    "        valid_indices = np.where(array > self.epsilon)[0]\n",
    "        if valid_indices.size > 0:\n",
    "            k_small = min(self.top_k, valid_indices.size)\n",
    "            top_indices = valid_indices[np.argpartition(-array[valid_indices], k_small - 1)[:k_small]]\n",
    "            return top_indices + offset\n",
    "        return [ ]\n",
    "    \n",
    "    def top_k_indices_per_area(self, inputs):\n",
    "        c_idx = [ ]\n",
    "        for a in range(self.n_areas):\n",
    "            ridx = a * self.nn\n",
    "            c_idx.extend(self.top_k_indices(inputs[ridx:ridx + self.nn], offset=ridx + self.ns))\n",
    "        return c_idx\n",
    "\n",
    "    def renormalize(self):\n",
    "        col_sums = self.W.sum(axis=0)\n",
    "        col_sums[col_sums < self.epsilon] = 1\n",
    "        self.W = self.W / col_sums\n",
    "    \n",
    "    def observe_sequence(self, sequence, beta, update=True, pc=False):\n",
    "        predictions = [ ]\n",
    "        p_idx, c_idx = [ ], [ ]\n",
    "        for t in range(len(sequence)):\n",
    "            # transfer synaptic input\n",
    "            self.inputs.fill(0)\n",
    "            c_idx.extend(sequence[t])\n",
    "            self.inputs += self.W[np.array(c_idx, dtype=np.int32), self.ns:].sum(axis=0)\n",
    "            # compute activations (top_k)\n",
    "            p_idx = c_idx\n",
    "            c_idx = self.top_k_indices_per_area(self.inputs)\n",
    "            c_out = [ c - self.ns for c in c_idx if self.ns <= c < self.ns + self.nn ]\n",
    "            predictions.append(c_out)\n",
    "            # plasticity\n",
    "            if (update):\n",
    "                for p in p_idx:\n",
    "                    for c in c_idx:\n",
    "                        # predictive coding rule (reinforce self.assemblies)\n",
    "                        sign = ((-1 if ((c - self.ns) not in self.assemblies[t]) else 1) if (self.ns <= c < self.ns + self.nn) else 1) if pc else 1\n",
    "                        self.W[p, c] *= (1 + sign * beta)\n",
    "        return predictions\n",
    "    \n",
    "    def hamming_similarity(self, a, b):\n",
    "        unmatch = len(set(a).symmetric_difference(set(b)))\n",
    "        return  1.0 - (unmatch / (2 * self.top_k))\n",
    "    \n",
    "    def get_assemblies(self, sequence):\n",
    "        assemblies = self.observe_sequence(sequence, beta=0, update=False)\n",
    "        return assemblies\n",
    "    \n",
    "    def train(self, sequence, rounds, beta, assemblies, pc=False):\n",
    "        self.assemblies = assemblies\n",
    "        for round in range(rounds):\n",
    "            # print(f'Round {round}:', end=' ')\n",
    "            predictions = self.observe_sequence(sequence, beta, update=True, pc=pc)\n",
    "            self.renormalize()  # renormalize brain after each training\n",
    "\n",
    "    def test(self, sequence, assemblies):\n",
    "        erased_sequence = [ sequence[t] if t == 0 else [] for t in range(len(sequence)) ]\n",
    "        predictions = self.observe_sequence(erased_sequence, 0, update=False)\n",
    "        metric = [ self.hamming_similarity(predictions[i], assemblies[i]) for i in range(len(assemblies)) ]\n",
    "        # print(metric)\n",
    "        return metric\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(n_sensory, length, top_k):\n",
    "    sequence = [ ]\n",
    "    for timestep in range(length):\n",
    "        sequence.append(list(np.random.choice(n_sensory, top_k, replace=False)))\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sequence = generate_sequence(n_sensory=2048, length=8, top_k=32)\n",
    "perturbation_step = 4\n",
    "perturbed_sequence = initial_sequence[:perturbation_step]\n",
    "perturbed_sequence.extend(generate_sequence(n_sensory=2048, length=8-perturbation_step, top_k=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.21875, 0.03125, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "learner = SequenceLearner(n_sensory=2048, n_neurons=2048, top_k=32, p=0.2, scaffold=False)\n",
    "\n",
    "initial_assemblies = learner.get_assemblies(initial_sequence)\n",
    "perturbed_assemblies = learner.get_assemblies(perturbed_sequence)\n",
    "learner.train(sequence=initial_sequence, rounds=10, beta=0.10, assemblies=initial_assemblies, pc=False)\n",
    "learner.train(sequence=perturbed_sequence, rounds=20, beta=0.10, assemblies=perturbed_assemblies, pc=False)\n",
    "score = learner.test(sequence=perturbed_sequence, assemblies=perturbed_assemblies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.90625, 0.875, 0.71875, 0.65625]\n"
     ]
    }
   ],
   "source": [
    "learner = SequenceLearner(n_sensory=2048, n_neurons=2048, top_k=32, p=0.2, scaffold=False)\n",
    "\n",
    "initial_assemblies = learner.get_assemblies(initial_sequence)\n",
    "perturbed_assemblies = learner.get_assemblies(perturbed_sequence)\n",
    "learner.train(sequence=initial_sequence, rounds=10, beta=0.10, assemblies=initial_assemblies, pc=True)\n",
    "learner.train(sequence=perturbed_sequence, rounds=20, beta=0.10, assemblies=perturbed_assemblies, pc=True)\n",
    "score = learner.test(sequence=perturbed_sequence, assemblies=perturbed_assemblies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.28125, 0.09375, 0.03125, 0.0]\n"
     ]
    }
   ],
   "source": [
    "learner = SequenceLearner(n_sensory=2048, n_neurons=2048, top_k=32, p=0.2, scaffold=True)\n",
    "\n",
    "initial_assemblies = learner.get_assemblies(initial_sequence)\n",
    "perturbed_assemblies = learner.get_assemblies(perturbed_sequence)\n",
    "learner.train(sequence=initial_sequence, rounds=10, beta=0.10, assemblies=initial_assemblies, pc=False)\n",
    "learner.train(sequence=perturbed_sequence, rounds=20, beta=0.10, assemblies=perturbed_assemblies, pc=False)\n",
    "score = learner.test(sequence=perturbed_sequence, assemblies=perturbed_assemblies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.96875, 0.9375, 0.5625, 0.28125]\n"
     ]
    }
   ],
   "source": [
    "learner = SequenceLearner(n_sensory=2048, n_neurons=2048, top_k=32, p=0.2, scaffold=True)\n",
    "\n",
    "initial_assemblies = learner.get_assemblies(initial_sequence)\n",
    "perturbed_assemblies = learner.get_assemblies(perturbed_sequence)\n",
    "learner.train(sequence=initial_sequence, rounds=10, beta=0.10, assemblies=initial_assemblies, pc=True)\n",
    "learner.train(sequence=perturbed_sequence, rounds=20, beta=0.10, assemblies=perturbed_assemblies, pc=True)\n",
    "score = learner.test(sequence=perturbed_sequence, assemblies=perturbed_assemblies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_csv(array):\n",
    "    for e in array:\n",
    "        print(e, end=', ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0, 1.0, 1.0, 1.0, 0.21875, 0.03125, 0.0, 0.0, \n",
      "1.0, 1.0, 1.0, 1.0, 0.90625, 0.875, 0.71875, 0.65625, \n",
      "1.0, 1.0, 1.0, 1.0, 0.28125, 0.09375, 0.03125, 0.0, \n",
      "1.0, 1.0, 1.0, 1.0, 0.96875, 0.9375, 0.5625, 0.28125, \n",
      "1.0, 1.0, 1.0, 1.0, 0.28125, 0.03125, 0.03125, 0.0625, \n",
      "1.0, 1.0, 1.0, 1.0, 0.9375, 0.9375, 0.875, 0.78125, \n",
      "1.0, 1.0, 1.0, 1.0, 0.3125, 0.1875, 0.03125, 0.03125, \n",
      "1.0, 1.0, 1.0, 1.0, 0.875, 0.90625, 0.8125, 0.4375, \n",
      "1.0, 1.0, 1.0, 1.0, 0.375, 0.09375, 0.09375, 0.0, \n",
      "1.0, 1.0, 1.0, 1.0, 0.78125, 0.5, 0.4375, 0.5, \n",
      "1.0, 0.96875, 0.84375, 0.5625, 0.03125, 0.0, 0.03125, 0.0, \n",
      "1.0, 1.0, 1.0, 1.0, 0.9375, 0.84375, 0.4375, 0.375, \n",
      "1.0, 1.0, 1.0, 0.96875, 0.125, 0.03125, 0.03125, 0.03125, \n",
      "1.0, 1.0, 1.0, 1.0, 0.96875, 0.90625, 0.78125, 0.4375, \n",
      "1.0, 1.0, 1.0, 1.0, 0.28125, 0.28125, 0.125, 0.0, \n",
      "1.0, 1.0, 1.0, 1.0, 0.96875, 0.90625, 0.53125, 0.28125, \n",
      "1.0, 1.0, 1.0, 1.0, 0.1875, 0.0, 0.0, 0.0, \n",
      "1.0, 1.0, 1.0, 1.0, 0.96875, 0.90625, 0.71875, 0.625, \n",
      "1.0, 1.0, 1.0, 1.0, 0.28125, 0.21875, 0.09375, 0.0625, \n",
      "1.0, 1.0, 1.0, 1.0, 0.9375, 0.90625, 0.625, 0.28125, \n"
     ]
    }
   ],
   "source": [
    "for iteration in range(5):\n",
    "    initial_sequence = generate_sequence(n_sensory=2048, length=8, top_k=32)\n",
    "    perturbation_step = 4\n",
    "    perturbed_sequence = initial_sequence[:perturbation_step]\n",
    "    perturbed_sequence.extend(generate_sequence(n_sensory=2048, length=8-perturbation_step, top_k=32))\n",
    "\n",
    "    learner = SequenceLearner(n_sensory=2048, n_neurons=2048, top_k=32, p=0.2, scaffold=False)\n",
    "    initial_assemblies = learner.get_assemblies(initial_sequence)\n",
    "    perturbed_assemblies = learner.get_assemblies(perturbed_sequence)\n",
    "    learner.train(sequence=initial_sequence, rounds=10, beta=0.10, assemblies=initial_assemblies, pc=False)\n",
    "    learner.train(sequence=perturbed_sequence, rounds=20, beta=0.10, assemblies=perturbed_assemblies, pc=False)\n",
    "    score = learner.test(sequence=perturbed_sequence, assemblies=perturbed_assemblies)\n",
    "    print_csv(score)\n",
    "\n",
    "    learner = SequenceLearner(n_sensory=2048, n_neurons=2048, top_k=32, p=0.2, scaffold=False)\n",
    "    initial_assemblies = learner.get_assemblies(initial_sequence)\n",
    "    perturbed_assemblies = learner.get_assemblies(perturbed_sequence)\n",
    "    learner.train(sequence=initial_sequence, rounds=10, beta=0.10, assemblies=initial_assemblies, pc=True)\n",
    "    learner.train(sequence=perturbed_sequence, rounds=20, beta=0.10, assemblies=perturbed_assemblies, pc=True)\n",
    "    score = learner.test(sequence=perturbed_sequence, assemblies=perturbed_assemblies)\n",
    "    print_csv(score)\n",
    "\n",
    "    learner = SequenceLearner(n_sensory=2048, n_neurons=2048, top_k=32, p=0.2, scaffold=True)\n",
    "    initial_assemblies = learner.get_assemblies(initial_sequence)\n",
    "    perturbed_assemblies = learner.get_assemblies(perturbed_sequence)\n",
    "    learner.train(sequence=initial_sequence, rounds=10, beta=0.10, assemblies=initial_assemblies, pc=False)\n",
    "    learner.train(sequence=perturbed_sequence, rounds=20, beta=0.10, assemblies=perturbed_assemblies, pc=False)\n",
    "    score = learner.test(sequence=perturbed_sequence, assemblies=perturbed_assemblies)\n",
    "    print_csv(score)\n",
    "\n",
    "    learner = SequenceLearner(n_sensory=2048, n_neurons=2048, top_k=32, p=0.2, scaffold=True)\n",
    "    initial_assemblies = learner.get_assemblies(initial_sequence)\n",
    "    perturbed_assemblies = learner.get_assemblies(perturbed_sequence)\n",
    "    learner.train(sequence=initial_sequence, rounds=10, beta=0.10, assemblies=initial_assemblies, pc=True)\n",
    "    learner.train(sequence=perturbed_sequence, rounds=20, beta=0.10, assemblies=perturbed_assemblies, pc=True)\n",
    "    score = learner.test(sequence=perturbed_sequence, assemblies=perturbed_assemblies)\n",
    "    print_csv(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
